{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (479725619.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install facenet-pytorch\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install facenet-pytorch\n",
    "pip install -v -e\n",
    "pip install -U openmim\n",
    "pip install mmcv\n",
    "pip install scikit-image\n",
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import skimage.io\n",
    "import io\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "# from scipy.misc import impip save\n",
    "from imageio import imwrite\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from skimage import data, img_as_float\n",
    "from skimage import exposure\n",
    "from PIL import Image, ImageEnhance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing MTCNN and InceptionResnetV1 \n",
    "mtcnn0 = MTCNN(image_size=240, margin=0, keep_all=False, min_face_size=40) # keep_all=False\n",
    "mtcnn = MTCNN(image_size=240, margin=0, keep_all=True, min_face_size=40) # keep_all=True\n",
    "# mtcnn = MTCNN(keep_all=True, device=device)\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\preprocessing\\image.py:1861: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mayan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\preprocessing\\image.py:1884: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = 'C:/Users/mayan/OneDrive/Desktop/CyberHack/train/train_images' \n",
    "os.listdir(data_dir)  \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "   \n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range = 40,\n",
    "        shear_range = 0.2,\n",
    "        zoom_range = 0.2,\n",
    "        horizontal_flip = True,\n",
    "        brightness_range = (0.5, 1.5),\n",
    "        featurewise_center=True,\n",
    "        samplewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=True,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.1,\n",
    "        channel_shift_range=0.1,\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,\n",
    "        rescale=None,\n",
    "        preprocessing_function=None)\n",
    "\n",
    "for image_class in os.listdir(data_dir): \n",
    "\n",
    "        i = 0\n",
    "\n",
    "        dir = \"C:/Users/mayan/OneDrive/Desktop/CyberHack/train/train_images/\"+image_class\n",
    "        aug = \"C:/Users/mayan/OneDrive/Desktop/CyberHack/train/train_images_augmented/\"+image_class\n",
    "\n",
    "        for batch in datagen.flow_from_directory(directory=dir, batch_size = 8,\n",
    "                          save_to_dir =aug, \n",
    "                          save_prefix ='aug', save_format ='jpg'):\n",
    "                i += 1\n",
    "                \n",
    "                if i > 100:\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from folder\n",
    "\n",
    "dataset = datasets.ImageFolder('C:/Users/mayan/OneDrive/Desktop/CyberHack/train/train_images_augmented') # photos folder path \n",
    "idx_to_class = {i:c for c,i in dataset.class_to_idx.items()} # accessing names of peoples from folder names\n",
    "\n",
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "loader = DataLoader(dataset, collate_fn=collate_fn)\n",
    "\n",
    "name_list = [] # list of names corrospoing to cropped photos\n",
    "embedding_list = [] # list of embeding matrix after conversion from cropped faces to embedding matrix using resnet\n",
    "\n",
    "for img, idx in loader:\n",
    "    face, prob = mtcnn0(img, return_prob=True) \n",
    "    if face is not None and prob>0.92:\n",
    "        emb = resnet(face.unsqueeze(0)) \n",
    "        embedding_list.append(emb.detach()) \n",
    "        name_list.append(idx_to_class[idx])        \n",
    "\n",
    "# dists = [[(e1 - e2).norm().item() for e2 in embedding_list] for e1 in embedding_list]\n",
    "# print(pd.DataFrame(dists, columns=name_list, index=name_list))\n",
    "\n",
    "# save data\n",
    "data = [embedding_list, name_list] \n",
    "torch.save(data, 'C:/Users/mayan/OneDrive/Desktop/CyberHack/data.pt') # saving data.pt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[0.99953413 0.99971145]\n",
      "[[120.55199 137.9341  225.98633 260.84952]\n",
      " [403.033   234.04762 504.48837 355.4253 ]]\n",
      "[0.99953413 0.99971145]\n",
      "120.55199 137.9341\n",
      "(120.55199, 137.9341) (225.98633, 260.84952)\n",
      "403.033 234.04762\n",
      "(403.033, 234.04762) (504.48837, 355.4253)\n",
      "False\n",
      "fail to grab frame, try again\n"
     ]
    }
   ],
   "source": [
    "from skimage.util import img_as_bool\n",
    "# from google.colab.patches import cv2_imshow\n",
    "from skimage import exposure\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import skimage.io\n",
    "import io\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "# from scipy.misc import impip save\n",
    "from imageio import imwrite\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from skimage import data, img_as_float\n",
    "from skimage import exposure\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "# Using webcam recognize face\n",
    "\n",
    "# COLOR BALANCER\n",
    "def apply_mask(matrix, mask, fill_value):\n",
    "    masked = np.ma.array(matrix, mask=mask, fill_value=fill_value)\n",
    "    return masked.filled()\n",
    "\n",
    "def apply_threshold(matrix, low_value, high_value):\n",
    "    low_mask = matrix < low_value\n",
    "    matrix = apply_mask(matrix, low_mask, low_value)\n",
    "\n",
    "    high_mask = matrix > high_value\n",
    "    matrix = apply_mask(matrix, high_mask, high_value)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def simplest_cb(img, percent):\n",
    "    assert img.shape[2] == 3\n",
    "    assert percent > 0 and percent < 100\n",
    "    half_percent = percent / 200.0\n",
    "    channels = cv2.split(img)\n",
    "\n",
    "    out_channels = []\n",
    "    for channel in channels:\n",
    "        assert len(channel.shape) == 2\n",
    "        # find the low and high precentile values (based on the input percentile)\n",
    "        height, width = channel.shape\n",
    "        vec_size = width * height\n",
    "        flat = channel.reshape(vec_size)\n",
    "\n",
    "        assert len(flat.shape) == 1\n",
    "        flat = np.sort(flat)\n",
    "        n_cols = flat.shape[0]\n",
    "\n",
    "        low_val  = flat[math.floor(n_cols * half_percent)]\n",
    "        high_val = flat[math.ceil( n_cols * (1.0 - half_percent))]\n",
    "\n",
    "        # saturate below the low percentile and above the high percentile\n",
    "        thresholded = apply_threshold(channel, low_val, high_val)\n",
    "        # scale the channel\n",
    "        normalized = cv2.normalize(thresholded, thresholded.copy(), 0, 255, cv2.NORM_MINMAX)\n",
    "        out_channels.append(normalized)\n",
    "\n",
    "    return cv2.merge(out_channels)\n",
    "def increase_brightness(img, value=30):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "\n",
    "    lim = 255 - value\n",
    "    v[v > lim] = 255\n",
    "    v[v <= lim] += value\n",
    "\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "    return img\n",
    "def contrastInput(img):\n",
    "    cv2.imshow(\"Original Image\",img)\n",
    "    # Contrast stretching\n",
    "    p2, p98 = np.percentile(img, (2, 98))\n",
    "    img_rescale = exposure.rescale_intensity(img, in_range=(p2, p98))\n",
    "    # imwrite('C:/Users/mayan/OneDrive/Desktop/Human-Face-Detection-in-Excessive-Dark-Image-master/CodePython/face/Contrast stretching/test1.jpg',img_rescale)\n",
    "    cv2.imshow(\"Contrast Image\",img_rescale)\n",
    "    #gray-scale image\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    cv2.imshow(\"Gray Image\",gray)\n",
    "    #Applying clahe (Contrast Limited Adaptive Histogram Equalization)\n",
    "    \n",
    "    # clahe = cv2.createCLAHE(clipLimit = 5)\n",
    "    clahe = cv2.createCLAHE(clipLimit =2.0, tileGridSize=(8,8))\n",
    "    final_img = clahe.apply(gray) + 30\n",
    "    cv2.imshow(\"Sharp Clahe\",final_img)\n",
    "    #Enhanced colored Image\n",
    "    l_channel, a, b = cv2.split(img)\n",
    "    # merge the CLAHE enhanced L-channel with the a and b channel\n",
    "    limg = cv2.merge((final_img,a,b))\n",
    "\n",
    "    # Converting image from LAB Color model to BGR color spcae\n",
    "    enhanced_img = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "    cv2.imshow(\"Enhanced Image\",enhanced_img)\n",
    "    #Equalization\n",
    "    equalized = cv2.equalizeHist(gray)\n",
    "    cv2.imshow(\"Equalized Image\",equalized)\n",
    "    # Equalization\n",
    "    img_eq = exposure.equalize_hist(gray)\n",
    "    # imwrite('C:/Users/mayan/OneDrive/Desktop/Human-Face-Detection-in-Excessive-Dark-Image-master/CodePython/face/Histogram equalization/test2.jpg',img_eq)\n",
    "    cv2.imshow(\"Hist Equalized Image\",img_eq)\n",
    "    # Adaptive Equalization\n",
    "    img_adapteq = exposure.equalize_adapthist(gray, clip_limit=0.03)\n",
    "    # imwrite('C:/Users/mayan/OneDrive/Desktop/Human-Face-Detection-in-Excessive-Dark-Image-master/CodePython/face/Adaptive equalization/test3.jpg',img_adapteq)\n",
    "    cv2.imshow(\"adapteq equalize\",img_adapteq)\n",
    "\n",
    "    #Brightness\n",
    "    imgBright = increase_brightness(img, value=20)\n",
    "    cv2.imshow(\"Bright image\",imgBright)\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) #convert it to hsv\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    v += 255\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    image = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "    cv2.imshow(\"Colorful Image\",image)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# loading data.pt file\n",
    "load_data = torch.load('C:/Users/mayan/OneDrive/Desktop/CyberHack/data.pt') \n",
    "embedding_list = load_data[0] \n",
    "name_list = load_data[1] \n",
    "\n",
    "cam = cv2.VideoCapture(\"C:/Users/mayan/OneDrive/Desktop/CyberHack/test/test_images/DCPMaam2.jpg\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    print(ret)\n",
    "    # if frame is not None:\n",
    "      # frame = cv2.resize(frame, (700, 500))\n",
    "\n",
    "    if not ret:\n",
    "        print(\"fail to grab frame, try again\")\n",
    "        break\n",
    "    # #Enhance Image Pixels\n",
    "    # sr = cv2.dnn_superres.DnnSuperResImpl_create()\n",
    "    # path = \"/content/gdrive/MyDrive/FaceRecog/facenet-pytorch/resolutionEnhancerModel/EDSR_x4.pb\"\n",
    "    # sr.readModel(path)\n",
    "    # sr.setModel(\"edsr\",4)\n",
    "    # result = sr.upsample(frame)\n",
    "    # # Resized image\n",
    "    # resized = cv2.resize(result,dsize=None,fx=4,fy=4)\n",
    "    # cv2.imshow(resized)\n",
    "    # dst = cv2.detailEnhance(frame, sigma_s=10, sigma_r=0.15)\n",
    "    # cv2.imshow(dst)\n",
    "    # contrastInput(frame)\n",
    "    img = Image.fromarray(frame)\n",
    "    img_cropped_list, prob_list = mtcnn(img, return_prob=True) \n",
    "    print(prob_list)\n",
    "    boxes, _ = mtcnn.detect(img)\n",
    "    print(boxes) \n",
    "    if img_cropped_list is not None:\n",
    "        boxes, _ = mtcnn.detect(img)\n",
    "        print(prob_list)        \n",
    "        for i, prob in enumerate(prob_list):\n",
    "            if prob>0.80:\n",
    "                emb = resnet(img_cropped_list[i].unsqueeze(0)).detach() \n",
    "                \n",
    "                dist_list = [] # list of matched distances, minimum distance is used to identify the person\n",
    "                \n",
    "                for idx, emb_db in enumerate(embedding_list):\n",
    "                    dist = torch.dist(emb, emb_db).item()\n",
    "                    dist_list.append(dist)\n",
    "\n",
    "                min_dist = min(dist_list) # get minumum dist value\n",
    "                min_dist_idx = dist_list.index(min_dist) # get minumum dist index\n",
    "                name = name_list[min_dist_idx] # get name corrosponding to minimum dist\n",
    "                \n",
    "                box = boxes[i] \n",
    "                \n",
    "                original_frame = frame.copy() # storing copy of frame before drawing on it\n",
    "                \n",
    "                if min_dist<0.90:\n",
    "                    print(box[0],box[1])\n",
    "                    frame = cv2.putText(frame, name+' '+str(min_dist), (int(box[0]), int(box[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),1, cv2.LINE_AA, False)\n",
    "                    print((box[0],box[1]) , (box[2],box[3]))\n",
    "                frame = cv2.rectangle(frame, (int(box[0]),int(box[1])) , (int(box[2]),int(box[3])), (255,0,0), 2)\n",
    "                \n",
    "    cv2.imshow(\"Recognized Face\",frame)\n",
    "    cv2.waitKey(0)\n",
    "    # cv2_imshow(frame)\n",
    "        \n",
    "    \n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256==27: # ESC\n",
    "        print('Esc pressed, closing...')\n",
    "        break\n",
    "        \n",
    "    elif k%256==32: # space to save image\n",
    "        print('Enter your name :')\n",
    "        name = input()\n",
    "        \n",
    "        # create directory if not exists\n",
    "        if not os.path.exists('photos/'+name):\n",
    "            os.mkdir('photos/'+name)\n",
    "            \n",
    "        img_name = \"photos/{}/{}.jpg\".format(name, int(time.time()))\n",
    "        cv2.imwrite(img_name, original_frame)\n",
    "        print(\" saved: {}\".format(img_name))\n",
    "        \n",
    "        \n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8afe5dd908a0c8b933273f6e6fad60f505637bcfc467eaa7c9b258be75f1dab6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
