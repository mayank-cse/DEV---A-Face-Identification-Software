{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO2Q9z4D2g/vOnmvPs/uujR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayank-cse/DEV---A-Face-Identification-Software/blob/main/testVideo3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDwPdQLL4pua"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facenet-pytorch\n",
        "!pip install -v -e\n",
        "# !pip install mmcv-full\n",
        "!pip install -U openmim\n",
        "# !mim install mmcv-full\n",
        "!pip install mmcv\n",
        "!pip install scikit-image\n",
        "!pip install -q streamlit\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "sLSgF2H145M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "0EalirukdCaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd .. \n",
        "%cd /content/gdrive/MyDrive/FaceRecog\n",
        "# !git clone https://github.com/timesler/facenet-pytorch.git"
      ],
      "metadata": {
        "id": "21UI8Vz5pJN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/open-mmlab/mmdetection.git\n",
        "%cd mmdetection\n",
        "!pip install -v -e ."
      ],
      "metadata": {
        "id": "3W0Tya3Q8sQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "q6wfzF5u87Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FACE DETECTION"
      ],
      "metadata": {
        "id": "onHKC76FF__K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from facenet_pytorch import MTCNN\n",
        "import torch\n",
        "import numpy as np\n",
        "import mmcv, cv2\n",
        "from PIL import Image, ImageDraw\n",
        "from IPython import display"
      ],
      "metadata": {
        "id": "Q237XpNr5Hqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Running on device: {}'.format(device))"
      ],
      "metadata": {
        "id": "-fMlsex99daK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mtcnn = MTCNN(keep_all=True, device=device)"
      ],
      "metadata": {
        "id": "ESjfEfGp9zpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video = mmcv.VideoReader('/content/maskPerson.jpeg')\n",
        "frames = [Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in video]\n",
        "\n",
        "display.Video('/content/gdrive/MyDrive/FaceRecog/face-recognition/Test/TestVideoShort.mp4', width=640)"
      ],
      "metadata": {
        "id": "Ffgxpcey93MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U kora\n",
        "# from kora.drive import upload_public\n",
        "# url = upload_public('/content/gdrive/MyDrive/FaceRecog/face-recognition/Test/TestVideoShort.mp4')\n",
        "# # then display it\n",
        "# from IPython.display import HTML\n",
        "# HTML(f\"\"\"<video src={url} width=500 controls/>\"\"\")"
      ],
      "metadata": {
        "id": "_YnUCm7e-l0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames_tracked = []\n",
        "for i, frame in enumerate(frames):\n",
        "    print('\\rTracking frame: {}'.format(i + 1), end='')\n",
        "    \n",
        "    # Detect faces\n",
        "    boxes, _ = mtcnn.detect(frame)\n",
        "    print(boxes)\n",
        "    if boxes is None:\n",
        "      continue\n",
        "    # Draw faces\n",
        "    frame_draw = frame.copy()\n",
        "    draw = ImageDraw.Draw(frame_draw)\n",
        "    for box in boxes:\n",
        "        draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
        "    \n",
        "    # Add to frame list\n",
        "    frames_tracked.append(frame_draw.resize((640, 360), Image.BILINEAR))\n",
        "print('\\nDone')"
      ],
      "metadata": {
        "id": "njwYYfxc_GC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = display.display(frames_tracked[0], display_id=True)\n",
        "i = 1\n",
        "try:\n",
        "    while True:\n",
        "        d.update(frames_tracked[i % len(frames_tracked)])\n",
        "        i += 1\n",
        "except KeyboardInterrupt:\n",
        "    pass"
      ],
      "metadata": {
        "id": "4HbbHOut_lbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dim = frames_tracked[0].size\n",
        "fourcc = cv2.VideoWriter_fourcc(*'FMP4')    \n",
        "video_tracked = cv2.VideoWriter('/content/maskPerson.jpeg', fourcc, 25.0, dim)\n",
        "for frame in frames_tracked:\n",
        "    video_tracked.write(cv2.cvtColor(np.array(frame), cv2.COLOR_RGB2BGR))\n",
        "video_tracked.release()"
      ],
      "metadata": {
        "id": "-5hHjlQNr4V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Augmentation"
      ],
      "metadata": {
        "id": "W2nbvw80u1oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import albumentations as A\n",
        "import numpy as np\n",
        "# from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "from PIL import Image\n",
        "from matplotlib import *\n",
        "from matplotlib.pyplot import *\n",
        "\n",
        "\n",
        "image = cv2.imread(\"/content/5.png\")\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "bboxes = [[13, 170, 224, 410]]\n",
        "\n",
        "# Pascal_voc (x_min, y_min, x_max, y_max), YOLO, COCO\n",
        "\n",
        "transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(width=1920, height=1080),\n",
        "        A.RandomCrop(width=1280, height=720),\n",
        "        A.Rotate(limit=40, p=0.9, border_mode=cv2.BORDER_CONSTANT),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.1),\n",
        "        A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.9),\n",
        "        A.OneOf([\n",
        "            A.Blur(blur_limit=3, p=0.5),\n",
        "            A.ColorJitter(p=0.5),\n",
        "        ], p=1.0),\n",
        "    ], bbox_params=A.BboxParams(format=\"pascal_voc\", min_area=2048,\n",
        "                                min_visibility=0.3, label_fields=[])\n",
        ")\n",
        "\n",
        "images_list = [image]\n",
        "saved_bboxes = [bboxes[0]]\n",
        "for i in range(15):\n",
        "    augmentations = transform(image=image, bboxes=bboxes)\n",
        "    augmented_img = augmentations[\"image\"]\n",
        "\n",
        "    if len(augmentations[\"bboxes\"]) == 0:\n",
        "        continue\n",
        "    visualize(augmented_img)\n",
        "    images_list.append(augmented_img)\n",
        "    saved_bboxes.append(augmentations[\"bboxes\"][0])"
      ],
      "metadata": {
        "id": "aRPSmWfWu5MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUGMENTATION"
      ],
      "metadata": {
        "id": "QVa0k5RHvVyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras\n",
        "\n",
        "import os\n",
        "\n",
        "data_dir = '/content/gdrive/MyDrive/FaceRecog/facenet-pytorch/data/test_images' \n",
        "os.listdir(data_dir)  \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "   \n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range = 40,\n",
        "        shear_range = 0.2,\n",
        "        zoom_range = 0.2,\n",
        "        horizontal_flip = True,\n",
        "        brightness_range = (0.5, 1.5),\n",
        "        featurewise_center=True,\n",
        "        samplewise_center=False,\n",
        "        featurewise_std_normalization=False,\n",
        "        samplewise_std_normalization=False,\n",
        "        zca_whitening=True,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.1,\n",
        "        channel_shift_range=0.1,\n",
        "        fill_mode='nearest',\n",
        "        cval=0.,\n",
        "        rescale=None,\n",
        "        preprocessing_function=None)\n",
        "\n",
        "for image_class in os.listdir(data_dir): \n",
        "\n",
        "        i = 0\n",
        "\n",
        "        dir = \"/content/gdrive/MyDrive/FaceRecog/facenet-pytorch/data/test_images/\"+image_class\n",
        "        aug = \"/content/gdrive/MyDrive/FaceRecog/facenet-pytorch/data/test_images_augmented/\"+image_class\n",
        "\n",
        "        for batch in datagen.flow_from_directory(directory=dir, batch_size = 8,\n",
        "                          save_to_dir =aug, \n",
        "                          save_prefix ='aug', save_format ='jpg'):\n",
        "                i += 1\n",
        "                \n",
        "                if i > 100:\n",
        "                        break"
      ],
      "metadata": {
        "id": "NS1t5RGP_1m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FACE RECOGNITION"
      ],
      "metadata": {
        "id": "JRKrleVXnn-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# importing libraries\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import time\n",
        "import os\n",
        "\n",
        "workers = 0 if os.name == 'nt' else 4"
      ],
      "metadata": {
        "id": "dQ7cfF1cnmzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Running on device: {}'.format(device))"
      ],
      "metadata": {
        "id": "2bUwA0econ_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mtcnn = MTCNN(\n",
        "    image_size=160, margin=0, min_face_size=20,\n",
        "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "xLfNun_RovnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)"
      ],
      "metadata": {
        "id": "coB5RSNFoywz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(x):\n",
        "    return x[0]\n",
        "# Reading data from the folder\n",
        "dataset = datasets.ImageFolder('/content/gdrive/MyDrive/FaceRecog/facenet-pytorch/data/test_images')\n",
        "dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()} #Accessing names of people from folder names\n",
        "loader = DataLoader(dataset, collate_fn=collate_fn, num_workers=workers)"
      ],
      "metadata": {
        "id": "00DoXVi_o3dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aligned = []\n",
        "names = []\n",
        "#x is image and y is index\n",
        "for x, y in loader:\n",
        "    x_aligned, prob = mtcnn(x, return_prob=True)\n",
        "    if x_aligned is not None:\n",
        "        print('Face detected with probability: {:8f}'.format(prob))\n",
        "        aligned.append(x_aligned)\n",
        "        names.append(dataset.idx_to_class[y])"
      ],
      "metadata": {
        "id": "UXsxYIWPp3Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aligned = torch.stack(aligned).to(device)\n",
        "embeddings = resnet(aligned).detach().cpu()"
      ],
      "metadata": {
        "id": "9SPQd3MXp_Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dists = [[(e1 - e2).norm().item() for e2 in embeddings] for e1 in embeddings]\n",
        "print(pd.DataFrame(dists, columns=names, index=names))\n",
        "\n",
        "# save data\n",
        "data = [aligned, names] \n",
        "torch.save(data, '/content/gdrive/MyDrive/FaceRecog/facenet-pytorch/data/data.pt') # saving data.pt file"
      ],
      "metadata": {
        "id": "ZzwzuTapqCUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "# Using webcam recognize face\n",
        "\n",
        "# loading data.pt file\n",
        "load_data = torch.load('/content/gdrive/MyDrive/FaceRecog/facenet-pytorch/data/data.pt') \n",
        "embedding_list = load_data[0] \n",
        "name_list = load_data[1] \n",
        "\n",
        "cam = cv2.VideoCapture('/content/mgtest.png') \n",
        "\n",
        "while True:\n",
        "    ret, frame = cam.read()\n",
        "    if not ret:\n",
        "        print(\"fail to grab frame, try again\")\n",
        "        break\n",
        "        \n",
        "    img = Image.fromarray(frame)\n",
        "    img_cropped_list, prob_list = mtcnn(img, return_prob=True) \n",
        "    print(mtcnn(img, return_prob=True))\n",
        "    if img_cropped_list is not None:\n",
        "        # print(1)\n",
        "        boxes, _ = mtcnn.detect(img)\n",
        "        print(boxes)\n",
        "        \n",
        "        # prob_list = prob_list.tolist()\n",
        "        \n",
        "        # print(enumerate(prob_list))\n",
        "        # for i, prob in enumerate(prob_list):\n",
        "        print((prob_list))\n",
        "        # prob_list = [[i] for i in prob_list]\n",
        "        # prob_list = list(prob_list)\n",
        "        for i, prob in enumerate(img_cropped_list):\n",
        "            # print(1)\n",
        "            # prob = prob_list[i]\n",
        "            print(prob)\n",
        "            prob = prob_list\n",
        "            if prob>0.90:\n",
        "                emb = resnet(img_cropped_list[i].unsqueeze(0)).detach() \n",
        "                \n",
        "                dist_list = [] # list of matched distances, minimum distance is used to identify the person\n",
        "                \n",
        "                for idx, emb_db in enumerate(embedding_list):\n",
        "                    dist = torch.dist(emb, emb_db).item()\n",
        "                    dist_list.append(dist)\n",
        "\n",
        "                min_dist = min(dist_list) # get minumum dist value\n",
        "                min_dist_idx = dist_list.index(min_dist) # get minumum dist index\n",
        "                name = name_list[min_dist_idx] # get name corrosponding to minimum dist\n",
        "                \n",
        "                box = boxes[i] \n",
        "                \n",
        "                original_frame = frame.copy() # storing copy of frame before drawing on it\n",
        "                \n",
        "                if min_dist<0.90:\n",
        "                    frame = cv2.putText(frame, name+' '+str(min_dist), (box[0],box[1]), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),1, cv2.LINE_AA)\n",
        "                \n",
        "                frame = cv2.rectangle(frame, (box[0],box[1]) , (box[2],box[3]), (255,0,0), 2)\n",
        "\n",
        "    cv2_imshow(frame)\n",
        "        \n",
        "    \n",
        "    k = cv2.waitKey(1)\n",
        "    if k%256==27: # ESC\n",
        "        print('Esc pressed, closing...')\n",
        "        break\n",
        "        \n",
        "    elif k%256==32: # space to save image\n",
        "        print('Enter your name :')\n",
        "        name = input()\n",
        "        \n",
        "        # create directory if not exists\n",
        "        if not os.path.exists('photos/'+name):\n",
        "            os.mkdir('photos/'+name)\n",
        "\n",
        "            \n",
        "        img_name = \"photos/{}/{}.jpg\".format(name, int(time.time()))\n",
        "        cv2.imwrite(img_name, original_frame)\n",
        "        print(\" saved: {}\".format(img_name))\n",
        "        \n",
        "        \n",
        "cam.release()\n",
        "cv2.destroyAllWindows()\n",
        "    "
      ],
      "metadata": {
        "id": "dE5I-E9I2Qpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TUNING THE MODEL"
      ],
      "metadata": {
        "id": "wRbqw-LsveWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization, training\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "2egzYQTNvdhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/gdrive/MyDrive/FaceRecog/facenet-pytorch/data/test_images'\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 8\n",
        "workers = 0 if os.name == 'nt' else 8"
      ],
      "metadata": {
        "id": "Z9EkdXoovnJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Running on device: {}'.format(device))"
      ],
      "metadata": {
        "id": "qnR-NEVbvpAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mtcnn = MTCNN(\n",
        "    image_size=160, margin=0, min_face_size=20,\n",
        "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "f0NhH2ydvo6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.ImageFolder(data_dir, transform=transforms.Resize((512, 512)))\n",
        "dataset.samples = [\n",
        "    (p, p.replace(data_dir, data_dir+ '_cropped'))\n",
        "        for p, _ in dataset.samples\n",
        "]\n",
        "        \n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    num_workers=workers,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=training.collate_pil\n",
        ")\n",
        "\n",
        "for i, (x, y) in enumerate(loader):\n",
        "    mtcnn(x, save_path=y)\n",
        "    print('\\rBatch {} of {}'.format(i + 1, len(loader)), end='')\n",
        "    \n",
        "# Remove mtcnn to reduce GPU memory usage\n",
        "del mtcnn"
      ],
      "metadata": {
        "id": "f_5XNr0OvrLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = InceptionResnetV1(\n",
        "    classify=True,\n",
        "    pretrained='vggface2',\n",
        "    num_classes=len(dataset.class_to_idx)\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "-MWKdutSvrJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n",
        "scheduler = MultiStepLR(optimizer, [5, 10])\n",
        "\n",
        "trans = transforms.Compose([\n",
        "    np.float32,\n",
        "    transforms.ToTensor(),\n",
        "    fixed_image_standardization\n",
        "])\n",
        "dataset = datasets.ImageFolder(data_dir + '_cropped', transform=trans)\n",
        "img_inds = np.arange(len(dataset))\n",
        "np.random.shuffle(img_inds)\n",
        "train_inds = img_inds[:int(0.8 * len(img_inds))]\n",
        "val_inds = img_inds[int(0.8 * len(img_inds)):]\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset,\n",
        "    num_workers=workers,\n",
        "    batch_size=batch_size,\n",
        "    sampler=SubsetRandomSampler(train_inds)\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    dataset,\n",
        "    num_workers=workers,\n",
        "    batch_size=batch_size,\n",
        "    sampler=SubsetRandomSampler(val_inds)\n",
        ")"
      ],
      "metadata": {
        "id": "_boqfagrx7BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "metrics = {\n",
        "    'fps': training.BatchTimer(),\n",
        "    'acc': training.accuracy\n",
        "}"
      ],
      "metadata": {
        "id": "8iBA5ii3x_-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer = SummaryWriter()\n",
        "writer.iteration, writer.interval = 0, 10\n",
        "\n",
        "print('\\n\\nInitial')\n",
        "print('-' * 10)\n",
        "resnet.eval()\n",
        "training.pass_epoch(\n",
        "    resnet, loss_fn, val_loader,\n",
        "    batch_metrics=metrics, show_running=True, device=device,\n",
        "    writer=writer\n",
        ")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print('\\nEpoch {}/{}'.format(epoch + 1, epochs))\n",
        "    print('-' * 10)\n",
        "\n",
        "    resnet.train()\n",
        "    training.pass_epoch(\n",
        "        resnet, loss_fn, train_loader, optimizer, scheduler,\n",
        "        batch_metrics=metrics, show_running=True, device=device,\n",
        "        writer=writer\n",
        "    )\n",
        "\n",
        "    resnet.eval()\n",
        "    training.pass_epoch(\n",
        "        resnet, loss_fn, val_loader,\n",
        "        batch_metrics=metrics, show_running=True, device=device,\n",
        "        writer=writer\n",
        "    )\n",
        "\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "SLajD6MJyELi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PYTORCH YT Video"
      ],
      "metadata": {
        "id": "zpzotnpF554Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import time\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import skimage.io\n",
        "import io\n",
        "\n",
        "# from scipy.misc import impip save\n",
        "from imageio import imwrite\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from skimage import data, img_as_float\n",
        "from skimage import exposure\n",
        "from PIL import Image, ImageEnhance\n",
        "\n"
      ],
      "metadata": {
        "id": "uMfpEOBW58qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# initializing MTCNN and InceptionResnetV1 \n",
        "\n",
        "mtcnn0 = MTCNN(image_size=240, margin=0, keep_all=False, min_face_size=40) # keep_all=False\n",
        "mtcnn = MTCNN(image_size=240, margin=0, keep_all=True, min_face_size=40) # keep_all=True\n",
        "# mtcnn = MTCNN(keep_all=True, device=device)\n",
        "resnet = InceptionResnetV1(pretrained='vggface2').eval() \n"
      ],
      "metadata": {
        "id": "6jg0wFrI6ASf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MULTIPLE TEST IMAGE GENERATOR"
      ],
      "metadata": {
        "id": "VLjjO8ifFFC7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZV1DR29mFDrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data from folder\n",
        "\n",
        "dataset = datasets.ImageFolder('/content/gdrive/MyDrive/FaceRecog/facenet-pytorch/data/test_images_augmented') # photos folder path \n",
        "idx_to_class = {i:c for c,i in dataset.class_to_idx.items()} # accessing names of peoples from folder names\n",
        "\n",
        "def collate_fn(x):\n",
        "    return x[0]\n",
        "\n",
        "loader = DataLoader(dataset, collate_fn=collate_fn)\n",
        "\n",
        "name_list = [] # list of names corrospoing to cropped photos\n",
        "embedding_list = [] # list of embeding matrix after conversion from cropped faces to embedding matrix using resnet\n",
        "\n",
        "for img, idx in loader:\n",
        "    face, prob = mtcnn0(img, return_prob=True) \n",
        "    if face is not None and prob>0.92:\n",
        "        emb = resnet(face.unsqueeze(0))\n",
        "        embedding_list.append(emb.detach()) \n",
        "        name_list.append(idx_to_class[idx])        \n",
        "\n",
        "# dists = [[(e1 - e2).norm().item() for e2 in embedding_list] for e1 in embedding_list]\n",
        "# print(pd.DataFrame(dists, columns=name_list, index=name_list))\n",
        "\n",
        "# save data\n",
        "data = [embedding_list, name_list] \n",
        "torch.save(data, 'data.pt') # saving data.pt file"
      ],
      "metadata": {
        "id": "N_uwv2K86DM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage.util import img_as_bool\n",
        "from google.colab.patches import cv2_imshow\n",
        "from skimage import exposure\n",
        "# Using webcam recognize face\n",
        "\n",
        "\n",
        "# COLOR BALANCER\n",
        "def apply_mask(matrix, mask, fill_value):\n",
        "    masked = np.ma.array(matrix, mask=mask, fill_value=fill_value)\n",
        "    return masked.filled()\n",
        "\n",
        "def apply_threshold(matrix, low_value, high_value):\n",
        "    low_mask = matrix < low_value\n",
        "    matrix = apply_mask(matrix, low_mask, low_value)\n",
        "\n",
        "    high_mask = matrix > high_value\n",
        "    matrix = apply_mask(matrix, high_mask, high_value)\n",
        "\n",
        "    return matrix\n",
        "\n",
        "def simplest_cb(img, percent):\n",
        "    assert img.shape[2] == 3\n",
        "    assert percent > 0 and percent < 100\n",
        "    half_percent = percent / 200.0\n",
        "    channels = cv2.split(img)\n",
        "\n",
        "    out_channels = []\n",
        "    for channel in channels:\n",
        "        assert len(channel.shape) == 2\n",
        "        # find the low and high precentile values (based on the input percentile)\n",
        "        height, width = channel.shape\n",
        "        vec_size = width * height\n",
        "        flat = channel.reshape(vec_size)\n",
        "\n",
        "        assert len(flat.shape) == 1\n",
        "        flat = np.sort(flat)\n",
        "        n_cols = flat.shape[0]\n",
        "\n",
        "        low_val  = flat[math.floor(n_cols * half_percent)]\n",
        "        high_val = flat[math.ceil( n_cols * (1.0 - half_percent))]\n",
        "\n",
        "        # saturate below the low percentile and above the high percentile\n",
        "        thresholded = apply_threshold(channel, low_val, high_val)\n",
        "        # scale the channel\n",
        "        normalized = cv2.normalize(thresholded, thresholded.copy(), 0, 255, cv2.NORM_MINMAX)\n",
        "        out_channels.append(normalized)\n",
        "\n",
        "    return cv2.merge(out_channels)\n",
        "def increase_brightness(img, value=30):\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    h, s, v = cv2.split(hsv)\n",
        "\n",
        "    lim = 255 - value\n",
        "    v[v > lim] = 255\n",
        "    v[v <= lim] += value\n",
        "\n",
        "    final_hsv = cv2.merge((h, s, v))\n",
        "    img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
        "    return img\n",
        "def contrastInput(img):\n",
        "    cv2_imshow(img)\n",
        "    # Contrast stretching\n",
        "    p2, p98 = np.percentile(img, (2, 98))\n",
        "    img_rescale = exposure.rescale_intensity(img, in_range=(p2, p98))\n",
        "    # imwrite('C:/Users/mayan/OneDrive/Desktop/Human-Face-Detection-in-Excessive-Dark-Image-master/CodePython/face/Contrast stretching/test1.jpg',img_rescale)\n",
        "    cv2_imshow(img_rescale)\n",
        "    #gray-scale image\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    cv2_imshow(gray)\n",
        "    #Applying clahe (Contrast Limited Adaptive Histogram Equalization)\n",
        "    \n",
        "    # clahe = cv2.createCLAHE(clipLimit = 5)\n",
        "    clahe = cv2.createCLAHE(clipLimit =2.0, tileGridSize=(8,8))\n",
        "    final_img = clahe.apply(gray) + 30\n",
        "    cv2_imshow(final_img)\n",
        "    #Enhanced colored Image\n",
        "    l_channel, a, b = cv2.split(img)\n",
        "    # merge the CLAHE enhanced L-channel with the a and b channel\n",
        "    limg = cv2.merge((final_img,a,b))\n",
        "\n",
        "    # Converting image from LAB Color model to BGR color spcae\n",
        "    enhanced_img = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
        "    cv2_imshow(enhanced_img)\n",
        "    #Equalization\n",
        "    equalized = cv2.equalizeHist(gray)\n",
        "    cv2_imshow(equalized)\n",
        "    # Equalization\n",
        "    img_eq = exposure.equalize_hist(gray)\n",
        "    # imwrite('C:/Users/mayan/OneDrive/Desktop/Human-Face-Detection-in-Excessive-Dark-Image-master/CodePython/face/Histogram equalization/test2.jpg',img_eq)\n",
        "    cv2_imshow(img_eq)\n",
        "    # Adaptive Equalization\n",
        "    img_adapteq = exposure.equalize_adapthist(gray, clip_limit=0.03)\n",
        "    # imwrite('C:/Users/mayan/OneDrive/Desktop/Human-Face-Detection-in-Excessive-Dark-Image-master/CodePython/face/Adaptive equalization/test3.jpg',img_adapteq)\n",
        "    cv2_imshow(img_adapteq)\n",
        "\n",
        "    #Brightness\n",
        "    imgBright = increase_brightness(img, value=20)\n",
        "    cv2_imshow(imgBright)\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) #convert it to hsv\n",
        "    h, s, v = cv2.split(hsv)\n",
        "    v += 255\n",
        "    final_hsv = cv2.merge((h, s, v))\n",
        "    image = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
        "    cv2_imshow(image)\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "# loading data.pt file\n",
        "load_data = torch.load('/content/data.pt') \n",
        "embedding_list = load_data[0] \n",
        "name_list = load_data[1] \n",
        "\n",
        "cam = cv2.VideoCapture('/content/3.mp4') \n",
        "while True:\n",
        "    ret, frame = cam.read()\n",
        "    print(ret)\n",
        "    # if frame is not None:\n",
        "    #   frame = cv2.resize(frame, (700, 500))\n",
        "\n",
        "    if not ret:\n",
        "        print(\"fail to grab frame, try again\")\n",
        "        break\n",
        "    \n",
        "    # #Enhance Image Pixels\n",
        "    # sr = cv2.dnn_superres.DnnSuperResImpl_create()\n",
        "    # path = \"/content/gdrive/MyDrive/FaceRecog/facenet-pytorch/resolutionEnhancerModel/EDSR_x4.pb\"\n",
        "    # sr.readModel(path)\n",
        "    # sr.setModel(\"edsr\",4)\n",
        "    # result = sr.upsample(frame)\n",
        "    # # Resized image\n",
        "    # resized = cv2.resize(result,dsize=None,fx=4,fy=4)\n",
        "    # cv2_imshow(resized)\n",
        "    # dst = cv2.detailEnhance(frame, sigma_s=10, sigma_r=0.15)\n",
        "    # cv2_imshow(dst)\n",
        "    # contrastInput(dst)\n",
        "    img = Image.fromarray(frame)\n",
        "    img_cropped_list, prob_list = mtcnn(img, return_prob=True) \n",
        "    print(prob_list)\n",
        "    boxes, _ = mtcnn.detect(img)\n",
        "    print(boxes) \n",
        "    if img_cropped_list is not None:\n",
        "        boxes, _ = mtcnn.detect(img)\n",
        "        print(prob_list)        \n",
        "        for i, prob in enumerate(prob_list):\n",
        "            if prob>0.80:\n",
        "                emb = resnet(img_cropped_list[i].unsqueeze(0)).detach() \n",
        "                \n",
        "                dist_list = [] # list of matched distances, minimum distance is used to identify the person\n",
        "                \n",
        "                for idx, emb_db in enumerate(embedding_list):\n",
        "                    dist = torch.dist(emb, emb_db).item()\n",
        "                    dist_list.append(dist)\n",
        "\n",
        "                min_dist = min(dist_list) # get minumum dist value\n",
        "                min_dist_idx = dist_list.index(min_dist) # get minumum dist index\n",
        "                name = name_list[min_dist_idx] # get name corrosponding to minimum dist\n",
        "                \n",
        "                box = boxes[i] \n",
        "                \n",
        "                original_frame = frame.copy() # storing copy of frame before drawing on it\n",
        "                \n",
        "                if min_dist<0.85:\n",
        "                    print(box[0],box[1])\n",
        "                    frame = cv2.putText(frame, name+' '+str(min_dist), (int(box[0]), int(box[1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0),1, cv2.LINE_AA, False)\n",
        "                    print((box[0],box[1]) , (box[2],box[3]))\n",
        "                frame = cv2.rectangle(frame, (int(box[0]),int(box[1])) , (int(box[2]),int(box[3])), (255,0,0), 2)\n",
        "                \n",
        "    \n",
        "    cv2_imshow(frame)\n",
        "    # cv2_imshow(frame)\n",
        "        \n",
        "    \n",
        "    k = cv2.waitKey(1)\n",
        "    if k%256==27: # ESC\n",
        "        print('Esc pressed, closing...')\n",
        "        break\n",
        "        \n",
        "    elif k%256==32: # space to save image\n",
        "        print('Enter your name :')\n",
        "        name = input()\n",
        "        \n",
        "        # create directory if not exists\n",
        "        if not os.path.exists('photos/'+name):\n",
        "            os.mkdir('photos/'+name)\n",
        "            \n",
        "        img_name = \"photos/{}/{}.jpg\".format(name, int(time.time()))\n",
        "        cv2.imwrite(img_name, original_frame)\n",
        "        print(\" saved: {}\".format(img_name))\n",
        "        \n",
        "        \n",
        "cam.release()\n",
        "cv2.destroyAllWindows()\n",
        "    "
      ],
      "metadata": {
        "id": "Tgodra2J6QXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hosting the application"
      ],
      "metadata": {
        "id": "oWbODdrVVA4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile webapp.py\n",
        "\n",
        "import streamlit as st\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "rec=cv2.face.LBPHFaceRecognizer_create()\n",
        "rec.read(\"trainingData.yml\")\n",
        "def detect_faces(our_image):\n",
        "    img = np.array(our_image.convert('RGB'))\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    # Detect faces\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "    # Draw rectangle around the faces\n",
        "    name='Unknown'\n",
        "    for (x, y, w, h) in faces:\n",
        "        # To draw a rectangle in a face\n",
        "        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 255, 0), 2)\n",
        "        id, uncertainty = rec.predict(gray[y:y + h, x:x + w])\n",
        "        print(id, uncertainty)\n",
        "\n",
        "        if (uncertainty< 53):\n",
        "            if (id == 1 or id == 3 or id == 5):\n",
        "                name = \"Nachiketa\"\n",
        "                cv2.putText(img, name, (x, y + h), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2.0, (0, 0, 255))\n",
        "        else:\n",
        "            cv2.putText(img, 'Unknown', (x, y + h), cv2.FONT_HERSHEY_COMPLEX_SMALL, 2.0, (0, 0, 255))\n",
        "\n",
        "\n",
        "    return img\n",
        "def main():\n",
        "    \"\"\"Face Recognition App\"\"\"\n",
        "\n",
        "    st.title(\"Streamlit Tutorial\")\n",
        "\n",
        "    html_temp = \"\"\"\n",
        "    <body style=\"background-color:red;\">\n",
        "    <div style=\"background-color:teal ;padding:10px\">\n",
        "    <h2 style=\"color:white;text-align:center;\">Face Recognition WebApp</h2>\n",
        "    </div>\n",
        "    </body>\n",
        "    \"\"\"\n",
        "    st.markdown(html_temp, unsafe_allow_html=True)\n",
        "\n",
        "    image_file = st.file_uploader(\"Upload Image\", type=['jpg', 'png', 'jpeg'])\n",
        "    if image_file is not None:\n",
        "        our_image = Image.open(image_file)\n",
        "        st.text(\"Original Image\")\n",
        "        st.image(our_image)\n",
        "\n",
        "    if st.button(\"Recognise\"):\n",
        "        result_img= detect_faces(our_image)\n",
        "        st.image(result_img)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "_VViqdFFIjKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install streamlit -q\n",
        "! pip install pyngrok\n",
        "! pip install --upgrade protobuf\n"
      ],
      "metadata": {
        "id": "m7CWcP6RVgjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "id": "RKDJ8vphVmBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.set_auth_token(\"2JOk1LlPWzFvC9AWEkfebDsaorh_7uqdTpaKTJCUMQju78Muc\") #ngrok.com"
      ],
      "metadata": {
        "id": "6oPUTMicVr6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correct\n",
        "!streamlit run webapp.py --server.port 80 &\n",
        "url = ngrok.connect(port='80')\n",
        "print(url)\n",
        "    "
      ],
      "metadata": {
        "id": "6Zkmtw8jVyzE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}